---
title: "Manuscript Analysis"
author: "David John Baker"
date: "30/09/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

This markdown documents the analyses used in the paper "Frequently Occuring Melodic Patterns are Easier to Recall".

Analyses here reflect comments after presenting this work at SMPC 2019, COGMIR 19, and at Marcus Pearce's Lab Meeting in Fall of 2019, and at the Ohio State University's Lab.

The text below is used in the Methods, Modeling and Results section of the manuscript.

# Data Import and Cleaning 

```{r}
library(readr)
library(dplyr)
library(stringr)
library(ggplot2)
library(viridis)
library(purrr)
library(broom)
library(scales)
library(tidyr)
library(reshape2)


# Demographic Data
demographic_data <- read_csv("data/aggregate_data/current_demo_table.csv")

# Single Tone Condition
single_tone_condition_table <- read_csv("data/aggregate_data/current_single_table.csv")

# Multi Tone Conditions 
multi_tone_response_table <- read_csv("data/aggregate_data/current_multi_table.csv")
```

# Methods 

```{r}
demographic_data %>%
  mutate(new_age = str_remove_all(string = age, pattern = "\\{\\\\Q0\\\\\\:\\\\")) %>%
  mutate(new_age = str_remove_all(string = new_age, pattern = "\\\\\\}")) %>%
  mutate(age = as.numeric(new_age)) %>%
  mutate(education = str_remove_all(string = education, pattern = "\\{\\\\Q0\\\\\\:\\\\")) %>%
  mutate(education = str_remove_all(string = education, pattern = "\\\\\\}")) %>%
  mutate(gender = str_remove_all(string = gender, pattern = "\\{\\\\Q0\\\\\\:\\\\")) %>%
  mutate(gender = str_remove_all(string = gender, pattern = "\\\\\\}")) %>%
  mutate(AP = str_remove_all(string = AP, pattern = "\\{\\\\Q0\\\\\\:\\\\")) %>%
  mutate(AP = str_remove_all(string = AP, pattern = "\\\\\\}")) %>%
  mutate(weeks_taking = str_remove_all(string = weeks_taking, pattern = "\\{\\\\Q0\\\\\\:\\\\")) %>%
  mutate(weeks_taking = str_remove_all(string = weeks_taking, pattern = "\\\\\\}")) %>%
  mutate(years_teaching = str_remove_all(string = years_teaching, pattern = "\\{\\\\Q0\\\\\\:\\\\")) %>%
  mutate(years_teaching = str_remove_all(string = years_teaching, pattern = "\\\\\\}")) %>%
  mutate(gender = str_to_upper(gender)) %>%
  mutate(gender = str_replace_all(string = gender, pattern = "\\F$", replacement = "FEMALE")) %>%
  select(subject_id,age, education, gender, AP, weeks_taking, years_teaching) -> d_table

d_table %>%
  print(n = nrow(d_table)) 

d_table$age %>% mean()
d_table$age %>% sd()
d_table$age %>% range()

d_table %>%
  group_by(gender) %>%
  tally()

```

Code for slides

```{r}

d_table %>%
  filter(gender %in% c("MALE", "FEMALE")) %>%
  ggplot(aes(x = age, fill = gender)) +
  geom_density(alpha = .5) +
  scale_fill_viridis(discrete = TRUE) +
#  guides(fill=FALSE) + 
  labs(title = "Age and Gender Distribution in Sample",
       x = "Age",
       y =  "Density",
       fill = "Gender") +
  theme_minimal()

```


# Modeling

Data from this experiment are reported following the three hypotheses listed above.

1. H1: Musical notes that occur more frequently in a musical corpora will be recalled more accurately and quickly than less frequently occurring musical patterns. 
2. H2: Exploratory analysis of computationally derived univariate features. 
3. H3: Mixed effects regression analysis modeling both accuracy and response time using theoretically motivated predictors of information content via computationally derived features. 
Participants were excluded from this study if they performed at chance levels in the single tone condition.
Chance level performance was taken to be indicitive that participants did not have the pre-requisite skills in order to partake in the experiment.
No participants were excluded from the study.

```{r}
single_tone_condition_table %>%
  group_by(subject) %>%
  summarise(mean_score = mean(score)) %>%
  arrange(mean_score)

multi_tone_response_table %>% 
  group_by(subject) %>%
  summarise(mean_score = mean(score)) %>%
  arrange(mean_score)
```

In order to examine the hypothesis that more frequently occurring musical notes will be recalled more accurately and quickly than less frequently occurring notes we adopt the following operationalizations. We define accuracy as the average percent of tones that were correctly identified across a participantâ€™s trial. Response time was measured in milliseconds to respond. Frequency of occurrence is modeled using zeroth order distribution frequency of occurrence in the MeloSol corpus, zeroth order frequency of starting tones in the MeloSol corpus, zeroth order frequency distribution in the Essen Folk Song Collection (Schaffrath 1995). Correlations between reaction time of trials, average accuracy, and the three measures of frequency of occurrence and presented in TABLE 1 with their frequency of distribution displayed in PANEL X. Correlations reported are Spearman rank order. As the experimental paradigm only consisted of a major key prime, scale degrees from minor keys are not included in the frequency count.

* Caclulate average score of each trial across all participants

* Accuracy ~ MeloSol Frequency Count 
* Accuray ~ MeloSol Starting Note Count 
* Accuray ~ Essen Frequnecy Count 

First check for no main effect of key. 

```{r}
#----------------------------------------------------------------------
# Check for No Effect of Key 
single_tone_condition_table %>%
  group_by(stimuli_together) %>%
  summarise(mean_score = mean(score)) %>%
  arrange(desc(mean_score)) %>%
  mutate(key = str_remove_all(stimuli_together, "-.*$")) %>%
  select(mean_score, key) -> effect_of_key

effect_of_key %>%
  ggplot(aes(x = key, y = mean_score)) +
  geom_boxplot()

# No Main Effect of Key 
effect_of_key_model <- aov(mean_score ~ key, data = effect_of_key)
summary(effect_of_key_model)

# Extract mean and sd RT for correct responses 
1066/1482

single_tone_condition_table %>%
  filter(score == 1) %>%
  group_by(scale_degree) %>%
  summarise(mean_rt = mean (rt),
            sd_rt = sd(rt)) %>%
  mutate(z_mean_rt = scale(mean_rt),
         z_mean_sd = scale(sd_rt),
         z_mean_rt_inv = z_mean_rt * -1) -> correct_single_tone_rt
  
```

Import count data for correlation plots

### Hypothesis I 

```{r}
#--------------------------------------------------
# MELOSOL TOTAL FREQUENCY
#--------------------------------------------------
# Total Counts of All Notes 
corpus_counts <- read_csv("data/aggregate_data/for_krum_multi_plot.csv")

# coding error from original 
corpus_counts <- corpus_counts %>%
  select(-scale_degree_f)

# coding error in original 
corpus_counts[corpus_counts$degree == "1-",1] <- "ra"

# 36,000 notes from MeloSol 
sum(corpus_counts$count)

# Set Factor For Graphing 
corpus_counts$scale_degree_f <- factor(corpus_counts$scale_degree, 
                                      levels = c("do","ra", "re", "me", "mi",
                                                 "fa", "fi","sol","le", "la", 
                                                 "te", "ti"))

corpus_counts %>%
  select(scale_degree, scale_degree_f, count, degree) %>%
  rename(melosol_total_count = count) -> melosol_freq_count_table

#--------------------------------------------------
# MELOSOL STARTING PITCH FREQUENCY
#--------------------------------------------------

# First Pitch (Huron, 2006, p. 65, 66) via MeloSol
huron_start_table <- read_csv("data/aggregate_data/huron_start_table.csv")

# 767 Melody Starts 
sum(huron_start_table$counts)

huron_start_table %>%
  rename(scale_degree_pc = scale_degree,
         melody_start_counts = counts) -> huron_start_table

huron_start_table$scale_degree <- c("do","ra", "re", "me", "mi",
                                    "fa", "fi","sol","le", "la", 
                                    "te", "ti")

huron_start_table$scale_degree_f <- factor(huron_start_table$scale_degree, 
                                           levels = c("do","ra", "re", "me", "mi",
                                                      "fa", "fi","sol","le", "la", 
                                                      "te", "ti"))

melosol_freq_count_table %>%
  left_join(huron_start_table) -> melosol_freq_count_table


#-----------------------------------------------------------
# ESSEN TOTAL COUNTS 
# Table of How Many times each SD occurs in Essen 
#------------------------------------------------------------
essen_table <- read_tsv("data/aggregate_data/essen_counts.tsv", col_names = FALSE)
essen_table <- essen_table %>%
  rename(count = X1,
         degree = X2)

melosol_freq_count_table <- melosol_freq_count_table %>%
  left_join(essen_table)

#-------------------------------------------------------------
# Create Correlations Between Avg Correct AND MeloSol (2) and Essen 
# Reaction time included as well ???

# Remove Minor Key Notes due to major prime in Experiment 

melosol_freq_count_table 

single_tone_condition_table %>%
  group_by(scale_degree) %>%
  summarise(mean_score = mean(score),
            sd_score = sd(score)) %>%
  arrange(desc(mean_score)) -> avg_single_tone_score_table

melosol_freq_count_table %>%
  left_join(avg_single_tone_score_table) %>%
  mutate(z_ms_total = scale(melosol_total_count),
         z_ms_start = scale(melody_start_counts),
         z_essen = scale(count),
          z_mean_score = scale(mean_score)) -> h1_table

colnames(h1_table)

h1_table$degree_f <- factor(h1_table$degree, levels = c("1", "1-","2-", "2","3-","3", "3+", "4", "4+", 
                                                    "5-", "5", "6-", "6", "6+", "7-", "7",  "7+"))

h1_table %>%
  left_join(correct_single_tone_rt) -> h1_table

h1_table %>%
  select(starts_with("z"))

#--------------------------------------------
# Calculate Correlations 
h1_table %>%
  # filter(degree != "1-") %>% # Enharmonic
  # filter(degree != "6-") %>% # Remove Minor Notes (Since Major Experiment Prime)
  # filter(degree != "6+") %>% # Remove Minor Notes (Since Major Experiment Prime)
  # filter(degree != "7+") %>%
  # filter(degree != "5-") %>%
  # filter(degree != "3+") %>%
  select(starts_with("z"), mean_score) %>%
  cor(use = "complete.obs", method = "spearman")

# Experimental Correlations 
cor.test(h1_table$z_mean_score, h1_table$z_ms_total, method = "spearman", alternative = "greater")
cor.test(h1_table$z_mean_score, h1_table$z_ms_start, method = "spearman", alternative = "greater")
cor.test(h1_table$z_mean_score, h1_table$z_essen, method = "spearman", alternative = "greater")

cor.test(h1_table$z_mean_score, h1_table$z_mean_rt, method = "spearman", alternative = "less")

# Corpus Correlations (Check MANUSCRIPT)
cor.test(h1_table$z_ms_total, h1_table$z_essen, method = "spearman", alternative = "greater")
cor.test(h1_table$z_ms_start, h1_table$z_essen, method = "spearman", alternative = "greater")
cor.test(h1_table$z_ms_start, h1_table$z_ms_total, method = "spearman", alternative = "greater")


h1_table %>%
  filter(!is.na(degree_f)) %>%
  ggplot(aes(x = degree_f)) +
  geom_point(aes(y = z_ms_total, color = "z_ms_total"),   group  = 1) +
  geom_line(aes(y = z_ms_total,  color = "z_ms_total"),  group = 1) +
  geom_point(aes(y = z_ms_start, color = "z_ms_start"),  group = 2) +
  geom_line(aes(y = z_ms_start,  color = "z_ms_start"),  group = 2) +
  geom_point(aes(y = z_essen,  color = "z_essen"),    group = 3) +
  geom_line(aes(y = z_essen,  color = "z_essen"),     group = 3) +
  geom_point(aes(y = z_mean_score, color = "z_mean_score"),group = 4) +
  geom_line(aes(y = z_mean_score, color = "z_mean_score"), group = 4) +
  theme_minimal() +
  labs(title = "Normalized Scale Degree Frequency and Mean Accuracy",
       x = "Scale Degree",
       y = "z score",
       color = "Legend") +
  scale_color_viridis(discrete = TRUE, labels = c(
                                    "Mean Accuracy of Experiment",
                                    "All Notes in Essen",
                                  "Starting Notes in MeloSol",
                                    "All Notes in MeloSol") ) -> mean_rt_corpus_figure

mean_rt_corpus_figure

# ggsave(filename = "figures/mean_rt_corpus.png",
#        mean_rt_corpus_figure,
#        width = 9, height = 4, units = "in")

#--------------------------------
# Reaction Time and Accuracy 
colors_h1_behavioral <- c("z_mean_score" = "black",
            "z_mean_rt_inv" = "blue")

# RT Correlations 
cor.test(h1_table$z_mean_rt_inv, h1_table$z_mean_score, method = "spearman", alternative = "greater")

h1_table %>%
  filter(!is.na(degree_f)) %>%
  ggplot(aes(x = degree_f)) +
  geom_point(aes(y = z_mean_score, color = "z_mean_score"),group = 1) + 
  geom_line(aes(y = z_mean_score, color = "z_mean_score"), group = 1) +
  geom_line(aes(y = z_mean_rt_inv, color = "z_mean_rt_inv"), group = 2) +
  geom_line(aes(y = z_mean_rt_inv, color = "z_mean_rt_inv"), group = 2) +
  theme_minimal() +
  labs(title = "Normalized Mean Accuracy and Inverted Reaction Time",
       x = "Scale Degree",
       y = "z score",
       color = "Legend") +
      scale_color_manual(values = colors_h1_behavioral,
                         labels = c("Mean Accuracy All Trials",
                                    "Inverted RT of Correct Trials")) -> mean_rt_corpus_figure_behavioral


mean_rt_corpus_figure_behavioral

# 
# ggsave(filename = "figures/mean_rt_corpus_behavioral.png",
# mean_rt_corpus_figure_behavioral, width = 9, height = 4, units = "in")

```

### Hypothesis II 

* Import in all theoretical variables of interest
* Plot each feature against mean performance of variable 


* IDYOM - Mean IC 
* IDYOM - Cumulitive IC 
* IDYOM -- Three Viewpoints

* FANTASTIC 

Three separate IDyOM models were run, the first using CPINT, the second CPINTREF, and the third using a combination of the CPINT and CPINTREF viewpoints. The information content as determined for each n-gram within each model was averaged.


```{r}
#======================================================================================================
# FANTASTIC DATA
fantastic_features <- read_csv("experiment_materials/new_stimuli/MelodyFeatures.csv")
#------------------------------------------------------------------------------------
# Clean FANTASTIC 
#--------------------------------------------------
fantastic_features %>%
  rename(stimulus = file.id) -> fantastic_features


fantastic_features$stimulus

fantastic_features$stimulus <- gsub(pattern = "m",replacement = "", fantastic_features$stimulus)
fantastic_features$stimulus <- gsub(pattern = "ono",replacement = "", fantastic_features$stimulus)
fantastic_features$stimulus <- gsub(pattern = "_$",replacement = " ", fantastic_features$stimulus)
fantastic_features$stimulus <- str_trim(fantastic_features$stimulus, side = "right")

fantastic_features$stimulus
#------------------------------------------------------------------------------------
# IDYOM Data 
cpint_model_1 <- read.delim("data/idyom_ouput/cpint_767.dat")
cpintref_model_2 <- read.delim("data/idyom_ouput/cpintfref_767.dat")
cpintcpintref_model_3 <- read.delim("data/idyom_ouput/idyom_data_m3.dat")

#----------------------------------------
# FUNCTIONS
#---------------------------------------
# Calculate Scale Degree 

create_referent <- function(keysig, modal){
  x <- ifelse(test = keysig >= 0, 
              yes = ((keysig * 7) + modal ) %% 12, 
              no = ((keysig * -5) + modal) %% 12  )
  x
}

# Chop Down Dataset
clean_raw_idyom <- function(idyom_output){
  idyom_output %>%
  select(melody.id, note.id, melody.name, phrase:pulses,
         mode:cpitch.entropy,probability, information.content, entropy) %>%
  mutate(referent = create_referent(keysig = keysig, modal = mode),
         scale_degree = (cpitch - referent) %% 12) 
    
}

cpint_model_1 <- clean_raw_idyom(cpint_model_1)
cpint_model_1$modelname <- "cpintmodel1"
cpintref_model_2 <- clean_raw_idyom(cpintref_model_2 )
cpintref_model_2$modelname <- "cpintrefmodel2"
cpintcpintref_model_3 <- clean_raw_idyom(cpintcpintref_model_3)
cpintcpintref_model_3$modelname <- "cpintcpintrefmodel3"

# Select Out All Patterns in Corpus 
# Import Set of Harcoded Functions to Get SDs from IDyOM data 
source(file = "analyses/find_idyom_gram_functions.R")

# This is a proof of concept to show it works
# You give it as first argument the pattern you want as series of integers
# You also give it the model to get those integer scale degrees from 

gram_finder(245, cpint_model_1) %>%
  select(melody.name, note.id, scale_degree, information.content, flag) %>%
  group_by(flag) %>% # Group every instance together 
  mutate(sum_ic = sum(information.content)) %>% # cumulative add all notes in pattern
  select(melody.name, sum_ic, flag) %>%
  distinct() %>% 
  ungroup(flag) %>% # 
  summarise(mean_total_ic_sums = mean(sum_ic))


#----------------------------------------------------
# Imports List of Stimuli Patterns used in Experiment with IDyOM notation 
helpful_list <- read_csv("data/aggregate_data/experimental_stimuli2.csv")

helpful_list$average_ic <- 0

# Stimuli List 
helpful_list_model_1 <- helpful_list %>%
  mutate(dataset =  "model1") %>%
  print(n = 41)

helpful_list_model_2 <- helpful_list %>%
  mutate(dataset =  "model2") %>%
  print(n = 41)

helpful_list_model_3 <- helpful_list %>%
  mutate(dataset =  "model3") %>%
  print(n = 41)

source(file = "corpus/run_idyom_ic_scripts/run_model_1_idyom_ic.R")
source(file = "corpus/run_idyom_ic_scripts/run_model_2_idyom_ic.R")
source(file = "corpus/run_idyom_ic_scripts/run_model_3_idyom_ic.R")

idyom_table <- rbind(helpful_list_model_1, helpful_list_model_2, helpful_list_model_3)

idyom_table_wide <- cbind(helpful_list[,c(1,2)],helpful_list_model_1[,c(3)], helpful_list_model_2[,c(3)], helpful_list_model_3[,c(3)])

colnames(idyom_table_wide) <- c("stimulus","idyom_notation","IDyOM IC cpint", "IDyOM IC cpintfref","IDyOM IC cpint + cpintfref")

idyom_table_wide

idyom_table %>%
  ggplot(aes(x = reorder(stimulus, average_ic), y = average_ic, color = dataset)) +
  geom_point() +
  coord_flip()

cogmir_counts <- read_csv("experiment_materials/stimuli_lists/cogmir_stimuli.csv")

# DELETE IF YOU DONT USE
krumhansl <- c(6.35,2.23,3.48,2.33,4.38,4.09,2.52,5.19,2.39,3.66,2.29,2.88)
scale_degree <- c("do","ra","re","me","mi","fa","fi","sol","le","la","te","ti")
krummy <- data.frame(krumhansl,scale_degree)
```

Tables Set, now time to run all univariate measures 


```{r}
# Make Response Table 
multi_tone_response_table %>%
  group_by(stimulus) %>%
  mutate(mean_score = mean(score)) %>%
  filter(trial_length >= 3) %>%
  select(stimulus, mean_score, trial_length) %>%
  ungroup(stimulus) %>%
  distinct()-> target_table
  
fantastic_features$stimulus

target_table %>%
  distinct(stimulus)

cogmir_counts %>%
  mutate(Pattern = str_replace_all(string = cogmir_counts$Pattern, pattern = " ", replacement = "_")) %>%
  rename(stimulus = Pattern) -> cogmir_counts

target_table %>%
  left_join(fantastic_features) %>%
  left_join(idyom_table_wide) %>%
  left_join(cogmir_counts) -> feature_table 


#-----------------------------------------------
# Large Feature Table Plot 

feature_table %>%
  select_if(is.numeric) %>%
  select(!starts_with("d")) %>%
  rename(`Frequency Count in Corpus` = Count, 
         `Number of Notes` = trial_length,
         `Quintile in Corpus` = quintile) %>%
  cor(use = "pairwise.complete.obs") %>%
  melt() %>%
  filter(Var1 == "mean_score") %>%
  filter(Var2 != "mean_score") %>%
  select(Var2, value) %>%
  rename(Feature = Var2, 
         Correlation = value) -> cor_plot_data

cor_plot_data %>% 
  mutate(`Feature Category` = "FIX ME") %>%
  mutate(`Feature Category` = case_when(
    str_detect(string = Feature, pattern = "^[a-z]") ~ "FANTASTIC",
    str_detect(string = Feature, pattern = "IDyOM") ~ "IDyOM",
    str_detect(string = Feature, pattern = "Number of Notes") ~ "Number of Notes",
    TRUE ~ "MeloSol",
  )) -> cor_plot_data

cor_plot_data %>%
  ggplot(aes(x = reorder(Feature, Correlation), y = Correlation, color = `Feature Category`)) + 
  geom_point() +
  scale_y_continuous(limits = c(-1,1), breaks = seq(-1, 1, .1)) +
  geom_hline(yintercept = .37, linetype = "longdash") +
  geom_hline(yintercept = -.37, linetype = "longdash") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature Correlation with Mean Accuracy",
       subtitle = "Results include 3, 5, 7, and 9 Note Trials",
       x = "Correlation Coeffecient",
       y = "Feature", 
       caption = "Dashed lines indicate p <.05 with N = 30") -> cor_plot_fig
  

cor_plot_fig

ggsave(filename = "figures/cor_plot_fig_h2.png",plot = cor_plot_fig, width = 9, height = 5, units = "in")


#--------------------------------------------
# Feature Table Linear Models 

# Fit LM predicting mean_score with each predictor
# Create table with: standardized beta, CI, R2, p value, DF

note_model <- lm(mean_score ~ trial_length, data = feature_table)
idyom3_model <- lm(mean_score ~ `IDyOM IC cpint + cpintfref`, data = feature_table )

summary(note_model)

anova(note_model, idyom3_model)

library(purrr)

feature_table %>%
  select_if(is.numeric) %>%
  select(-mean_score) %>%
  select(!starts_with("d")) %>%
  map(~lm(feature_table$mean_score ~ .x, data = feature_table)) %>%
  map(summary) %>%
  map_dbl("r.squared") %>%
  tidy() %>%
  rename(R2 = x) -> r2feature_table

feature_table %>%
  select_if(is.numeric) %>%
  select(-mean_score) %>%
  select(!starts_with("d")) %>%
  map(~lm(feature_table$mean_score ~ .x, data = feature_table)) %>%
  map(summary) %>%
  map_df(glance) %>%
  print(n = 100) -> glance_table

univariate_table <- cbind(r2feature_table, glance_table)

univariate_table

univariate_table %>%
  select(-r.squared) %>%
  mutate(across(is.numeric, round, 3)) %>%
  htmlTable::htmlTable()
  
```

Create Cowplots 

```{r}
#======================================================================================================
# Make Univariate Feature Cow-Plot
#--------------------------------------------------


# Length Plot
feature_table %>%
  ggplot(aes(x = len, y = mean_score)) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Number of Notes", y = "Average Percentage Correct",
       title = "Number of Notes\n") +
  scale_y_continuous(labels = percent) -> len_plot 
  
# FANTASTIC

feature_table %>%
  ggplot(aes(x = step.cont.loc.var, y = mean_score)) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Stepwise Countour Local Variation", 
       y = "Average Percentage Correct",
       title = "FANTASTIC\nStepwise Contour Local Variation") +
  scale_y_continuous(labels = percent) -> sclv_plot 

sclv_plot  

feature_table %>%
  ggplot(aes(x = mean.entropy, y = mean_score)) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Mean Entropy", y = "Average Percentage Correct",
       title = "FANTASTIC\nMean Entropy") +
  scale_y_continuous(labels = percent) -> mean_entropy_plot 

mean_entropy_plot  

feature_table %>%
  ggplot(aes(x = tonalness, y = mean_score)) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Tonalness", y = "Average Percentage Correct",
       title = "FANTASTIC\nTonalness") +
  scale_y_continuous(labels = percent) -> tonalness_plot 

tonalness_plot  


#-----------------------------------
# IDyOM Plots 

feature_table %>%
  ggplot(aes(x = `IDyOM IC cpint`, y = mean_score )) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Average Information Content", 
       y = "Average Percentage Correct",
       title = "IDyOM\nChromatic Interval Viewpoint") +
  scale_y_continuous(labels = percent) -> idyom_cpint

idyom_cpint

feature_table %>%
  ggplot(aes(x = `IDyOM IC cpintfref`, y = mean_score)) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Average Information Content", 
       y = "Average Percentage Correct",
       title = "IDyOM\nChromatic Pitch Interval Viewpoint") +
  scale_y_continuous(labels = percent) -> idyom_cpintfref 

idyom_cpintfref

feature_table %>%
  ggplot(aes(x = `IDyOM IC cpint + cpintfref`, y = mean_score)) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Average Information Content", y = "Average Percentage Correct",
       title = "IDyOM\nCombined Viewpoints") +
  scale_y_continuous(labels = percent) -> idyom_combined_viewpoints 

idyom_combined_viewpoints
#-------------------------------------------------------------------------
# MeloSol

feature_table %>%
  ggplot(aes(x = log(Count), y = mean_score )) +
  geom_point() +
  theme_minimal() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Count in MeloSol Corpus", 
       y = "Average Percentage Correct",
       title = "Frequency Count\nMeloSol Corpus (Log Scale)") +
  scale_y_continuous(labels = percent) -> melosol_freq_count_acc

melosol_freq_count_acc

#----------------------------------------------------------------------------
# Number of Notes
len_plot

sclv_plot
mean_entropy_plot
tonalness_plot  

idyom_cpint
idyom_cpintfref
idyom_combined_viewpoints

melosol_freq_count_acc
#--------------------------------------------------

cowplot::plot_grid(
  len_plot,
  sclv_plot,
  mean_entropy_plot,
  tonalness_plot  ,
  idyom_cpint,
  idyom_cpintfref,
  idyom_combined_viewpoints,
  melosol_freq_count_acc, ncol = 4, nrow = 2
) -> cow2

cow2

#ggsave(filename = "figures/cowplot.png",plot = cow2, height = 20, width = 40, units = "cm")

```


## H3

Linear Mixed Effects Model 



```{r}
# Calculate Score Per Trial Per Participant
multi_tone_response_table %>%
  group_by(subject, stimulus) %>%
  mutate(scored_trial = mean(score)) %>%
  select(subject, stimulus, scored_trial, trial_length) %>%
  ungroup(subject, stimulus) %>%
  distinct() -> per_trial_per_participant

# Only Multi Conditions
per_trial_per_participant %>%
  filter(trial_length >= 3) %>%
  left_join(feature_table) -> response_feature_table

response_feature_table %>% View() 

library(lme4)

model_number_of_notes <- lmer(scored_trial ~ len + (1+ len | subject), data = response_feature_table)

summary(model_number_of_notes)


model_idyom_3 <- lmer(scored_trial ~  `IDyOM IC cpint + cpintfref`+ (1+ `IDyOM IC cpint + cpintfref` | subject), data = response_feature_table)

summary(model_idyom_3)

anova(model_number_of_notes, model_idyom_3)

# R2s for table here, not SJ Plot 
MuMIn::r.squaredGLMM(model_number_of_notes)
MuMIn::r.squaredGLMM(model_idyom_3)

sjPlot::tab_model(model_number_of_notes, model_idyom_3)

# 0 = -0.02x + 1.02
# Y = -0.02(0) + 1.02


102/.02

```

Discussion Calculations

```{r}
summary(note_model)
.92658/.03209

summary(model_idyom_3)
1.024016/0.189

single_tone_condition_table %>%
  summarise(mean = mean(score))


```



```{r}

View(demographic_data)

multi_tone_response_table %>%
  group_by(subject, trial_length) %>%
  summarise(mean_rt = mean(rt)) %>%
  ggplot(aes(x = trial_length, y = mean_rt)) +
  geom_point() +
  facet_wrap(~subject)
  

# Scratch 
# Only Complete Trials 
multi_tone_response_table %>%
  group_by(subject, stimulus) %>%
  mutate(trial_score = mean(score)) %>%
  filter(trial_score == 1) %>%
  ungroup(subject, stimulus) %>%
  group_by(serial_order, stimulus) %>%
  mutate(mean_rt = mean(rt)) %>%
  ggplot(aes(x = serial_order, y = mean_rt, color = stimulus)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~trial_length)

  
# how many complete trials do i have
# Does that change over n increase 

multi_tone_response_table %>%
  mutate(unique_trial = paste(subject, stimulus)) %>%
  left_join(trial_totaller) %>%
  group_by(unique_trial) 



multi_tone_response_table %>%
  filter(trial_length == 3) %>%
  group_by(serial_order, stimulus) %>%
  mutate(mean_score = mean(score),
         mean_rt = mean(rt)) %>%
  ggplot(aes(x = serial_order, y = mean_score, color = stimulus)) +
  geom_point() +
  geom_smooth()


multi_tone_response_table %>%
  filter(trial_length == 3) %>%
  group_by(serial_order, stimulus) %>%
  mutate(mean_score = mean(score),
         mean_rt = mean(rt)) %>%
  ggplot(aes(x = serial_order, y = mean_rt, color = stimulus)) +
  geom_point() +
  geom_smooth()

```



